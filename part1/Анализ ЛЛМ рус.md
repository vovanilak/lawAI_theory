https://arxiv.org/abs/2307.06435 
A. Предварительно обученные ЛЛМ
Здесь мы приводим краткое описание различных известных предварительно обученных LLM, которые совершили значительные открытия, изменившие ход исследований и разработок в области НЛП. Эти LLM значительно улучшили производительность в областях NLU и NLG, а также широко используются для решения последующих задач.
1. Общее назначение:
1.1 T5 [10]: Модель кодировщика-декодировщика, использующая унифицированное обучение "текст-текст" для всех задач НЛП, показана на рис. 7. В T5 нормализация слоев вынесена за пределы остаточного пути в обычной трансформаторной модели [62]. В качестве задачи предварительного обучения в ней используется моделирование языка по маске, при котором вместо отдельных масок для каждой лексемы используется единая маска (последовательные лексемы). Такой тип маскирования ускоряет обучение, поскольку позволяет получить более короткие последовательности. После предварительного обучения модель настраивается с помощью слоев-адаптеров [102] для последующих задач.
1.2 GPT-3 [6]: Архитектура GPT-3 аналогична архитектуреGPT-2 [5], но с плотным и разреженным вниманием в слоях трансформации, подобно Sparse Transformer [63]. Она показывает, что крупные модели могут обучаться на больших объемах партий с меньшей скоростью обучения; чтобы определить размер партии во время обучения, GPT-3 использует шкалу градиентного шума, как в [103]. В целом, GPT-3 увеличивает параметры модели до 175B, показывая, что производительность больших языковых моделей улучшается с увеличением масштаба и является конкурентоспособной по сравнению с моделями с точной настройкой.
1.3 mT5 [11]: Многоязычная модель T5 [10], обученная на наборе данных mC4 со 101 языком. Набор данных извлекается из общедоступного common crawl scrape. Чтобы избежать чрезмерной или недостаточной подгонки для одного языка, в модели mT5 используется процедура выборки данных для отбора образцов из всех языков. В статье предлагается использовать небольшое количество предтренировочных наборов данных, включающих все языки, при тонкой настройке для задачи, использующей англоязычные данные. Это позволяет модели генерировать корректные неанглийские результаты.
1.4 PanGu-α [104]: Авторегрессионная модель, которая имеет слой aquery в конце стандартных слоев трансформатора, примеры показаны на рис. 8, с целью предсказания следующей лексемы. Ее структура аналогична трансформаторному слою, но с дополнительной вставкой для следующей позиции в механизме внимания, представленной в уравнении 7.
1,5 CPM-2 [12]: Cost-efficient Pre-trained LanguageModels (CPM-2) предварительно обучает двуязычные (английский и китайский) 11B и 198B модели смеси экспертов (MoE) на наборе данных WuDaoCorpus [105]. В процессе токенизации токены с пробелами "_" удаляются в токенизаторе частей предложения. Модели обучаются с наследованием знаний, начиная только с китайского языка на первом этапе и затем добавляя английские и китайские данные. Эта обученная модель многократно дублируется для инициализации модели 198B MoE. Более того, чтобы использовать модель для последующих задач, CPM-2 экспериментировал как с полной тонкой настройкой, так и с тонкой настройкой подсказок, как в [106], где обновляются только параметры, связанные с подсказками, путем вставки подсказок в различные позиции: впереди, посередине и сзади. В CPM-2 также предлагается INFMOE, фреймворк, экономящий память, со стратегией динамической разгрузки параметров на центральный процессор для выводов в масштабе 100 ББ. Он совмещает перемещение данных с вычислением выводов, что позволяет сократить время вычислений.
1.6 ERNIE 3.0 [107]: ERNIE 3.0 черпает вдохновение в многозадачном обучении для построения модульной архитектуры, используя в качестве основы трансформер-XL [108]. Универсальный модуль представления является общим для всех задач и служит базовым блоком для модулей представления конкретных задач, которые совместно обучаются для понимания естественного языка, генерации естественного языка и извлечения знаний. Этот LLM ориентирован в первую очередь на китайский язык, утверждает, что обучается на самых больших китайских текстовых корпорациях для обучения LLM, и достиг передовых результатов в 54 китайских задачах НЛП.
1.7 Jurassic-1 [109]: Пара авторегрессионных моделей языка, включающая 7B-параметрическую модель J1-Large и 178B-параметрическую модель J1-Jumbo. Обучающий словарь Jurassic-1 состоит из фрагментов слов, полных слов и многословных выражений без границ слов, где возможные внесловарные экземпляры интерпретируются как байты Unicode. По сравнению с аналогами GPT-3, модели Jurassic-1 используют более сбалансированную архитектуру самовнимания по соотношению глубины и ширины [110] и улучшенный токенизатор для более быстрого предсказания на основе более широких ресурсов, достигая сопоставимой производительности в задачах обучения с нулевым результатом и более высокой производительности в задачах обучения с несколькими результатами, учитывая возможность подачи большего количества примеров в качестве подсказки.
1.8 HyperCLOVA [111]: Модель корейского языка с архитектурой GPT-3.
1,9 Юань 1,0 [112]: Обучен на китайском корпусе с 5 ТБ высококачественного текста, собранного из Интернета. Для обработки исходных данных с помощью методов грубой и тонкой фильтрации разработана система фильтрации массивных данных (MDFS), построенная на базе Spark. Для ускорения обучения Yuan 1.0 с целью экономии энергозатрат и выбросов углекислого газа в архитектуру и обучение были включены различные факторы, повышающие производительность распределенного обучения: увеличение количества скрытых размеров улучшает производительность конвейерного и тензорного параллелизма, более крупные микропакеты улучшают производительность конвейерного параллелизма, а более высокий размер глобального пакета улучшает производительность параллелизма данных. На практике модель Yuan 1.0 хорошо справляется с задачами классификации текстов, Winograd Schema, выводами на естественном языке и пониманием прочитанного.
1.10 Gopher [113]: Семейство моделей Gopher варьируется от 44M до 280B параметров для изучения влияния масштаба на производительность LLM. Модель с 280B параметрами выигрывает у GPT3 [6], Jurrasic-1 [109], MT-NLG [114] и других на 81% оцениваемых задач.
1.11 ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan расширяет возможности ERNIE 3.0 за счет обучения более крупной модели с 26-кратным количеством параметров. Эта более крупная модель превзошла другие современные модели в 68 задачах НЛП. LLM создают текст с неверными фактами. Для того чтобы контролировать генерируемый текст с фактической последовательностью, ERNIE 3.0 Titan добавляет к своей многозадачной системе обучения еще одну задачу - "Достоверные и контролируемые поколения". Она вводит дополнительные потери при самоконтролируемом аверсальном и контролируемом языковом моделировании на этапе предварительного обучения, что позволяет ERNIE 3.0 Titan опередить другие LLM в их оценках набора задач Factual QA, выбранных вручную.
1.12 GPT-NeoX-20B [115]: Авторегрессионная модель, во многом повторяющая GPT-3 с некоторыми отклонениями в архитектуре, обученная на наборе данных Pile без дедупликации данных. GPT-NeoX имеет параллельное внимание и подачу вперед слоев в блоке трансформации, представленную в уравнении 8, что увеличивает пропускную способность на 15 %. Он использует поворотное позиционное встраивание [66], применяя его только к 25 % размерности вектора встраивания, как в [116]. Это сокращает объем вычислений без снижения производительности. В отличие от GPT-3, использующего плотные и разреженные слои, GPT-NeoX-20B использует только плотные слои. Настройка гиперпараметров в таком масштабе затруднена, поэтому модель выбирает гиперпараметры из метода [6] и интерполирует значения между моделями 13B и 175B для модели 20B. Обучение модели распределяется между графическими процессорами с использованием тензорного и конвейерного параллелизма.
1.13 OPT [14]: Это клон GPT-3, разработанный с целью создания модели с открытым исходным кодом, повторяющей производительность GPT-3. Обучение OPT использует динамическое масштабирование потерь[117] и перезапускается с более ранней контрольной точки с более низкой скоростью обучения, когда наблюдается расхождение потерь. В целом, производительность моделей OPT-175B сопоставима с модельюGPT3-175B.
1.14 BLOOM [13]: Модель каузального декодера, обученная на корпусе ROOTS с целью открытого доступа к LLM. Архитектура BLOOM показана на рисунке 9, но в ней есть такие отличия, как позиционное встраивание ALiBi, дополнительный слой нормализации после слоя встраивания, предложенный библиотекой bitsandbytes1. Эти изменения стабилизируют процесс обучения и улучшают последующую производительность.
1.15 GLaM [118]: Generalist Language Model (GLaM) представляет собой семейство языковых моделей, использующих структуру смеси экспертов (MoE) с редкой активацией декодера [119], [120]. Чтобы увеличить емкость модели при сокращении вычислений, эксперты активируются разреженно, когда для обработки каждой входной лексемы используются только два лучших эксперта. Самая большая модель GLaM, GLaM (64B/64E), примерно в 7 раз больше, чем GPT3 [6], при этом на каждый входной токен активируется только часть параметров. Самая большая модель GLaM (64B/64E) достигает лучших общих результатов по сравнению с GPT-3, потребляя при этом лишь треть энергии обучения GPT-3.
1.16 MT-NLG [114]: Каузальный декодер 530B, основанный на архитектуреGPT-2, которая примерно в 3 раза превышает параметры модели GPT-3. MT-NLG обучается на отфильтрованных высококачественных данных, собранных из различных публичных наборов данных, и смешивает различные типы наборов данных в одном пакете, что превосходит GPT-3 по ряду оценок.
1.17 Chinchilla [121]: Каузальный декодер, обученный на том же наборе данных, что и Gopher [113], но с немного другим распределением выборки данных (выборка из MassiveText). Архитектура модели аналогична той, что использовалась в Gopher, за исключением оптимизатора AdamW вместо Adam. Chinchilla выявляет соотношение, согласно которому размер модели должен удваиваться при каждом удвоении количества обучающих лексем. Более 400 языковых моделей от 70 миллионов до 16 миллиардов параметров на 5-500 миллиардах лексем были обучены для получения оценок оптимального обучения при заданном бюджете. Авторы обучают модель 70B с тем же бюджетом вычислений, что и Gopher (280B), но с в 4 раза большим количеством данных. После тонкой настройки она превосходит модели Gopher [113], GPT-3 [6] и другие на различных последующих задачах.
1.18 AlexaTM [122]: Модель кодера-декодера, в которой веса кодера и вкрапления декодера инициализируются предварительно обученным кодером для ускорения обучения. Кодер остается замороженным в течение первых 100 тыс. шагов, а затем размораживается для конечного обучения. Модель обучается на комбинации задач denoising и causal language modeling (CLM), конкатенируя токен [CLM] в начале для переключения режимов. Во время обучения задача CLM применяется в течение 20 % времени, что улучшает эффективность обучения в контексте.
1.19 PaLM [15]: Каузальный декодер с параллельным вниманием и слоями обратной связи, аналогичный уравнению 8, ускоряющий обучение в 15 раз. Дополнительные изменения по сравнению с обычной трансформационной моделью включают активацию SwiGLU, RoPE-эмбеддинги, многозапросное внимание, которое экономит вычислительные затраты при декодировании, и общие эмбеддинги вход-выход. В процессе обучения были замечены пики потерь, и для их устранения обучение модели было перезапущено с контрольной точки на 100 шагов раньше, пропуская 200-500 батчей вокруг пика. Кроме того, было обнаружено, что модель запоминает около 2,4% обучающих данных при масштабе модели 540B, в то время как для моделей меньшего размера это число было ниже.PaLM-2 [123]: Уменьшенный мультиязычный вариант PaLM, обученный за большее количество итераций на наборе данных лучшего качества. PaLM-2 демонстрирует значительные улучшения по сравнению с PaLM, при этом снижая затраты на обучение и вывод за счет меньшего размера. Чтобы уменьшить токсичность и запоминание, он добавляет специальные лексемы к части данных предварительного обучения, что показывает снижение количества вредных реакций.
1.20 U-PaLM [124]: Этот метод обучает PaLM за 0,1% дополнительных вычислений с целью UL2 (также называемой UL2Restore) [89], используя тот же набор данных, и значительно превосходит базовый уровень на различных задачах НЛП, включая zero-shot, fewshot, commonsense reasoning, CoT и т.д. Обучение с помощью UL2R заключается в преобразовании каузального декодера PaLM в некаузальный декодер PaLM и использовании функций потерь 50% последовательного обесценивания, 25% регулярного обесценивания и 25% экстремального обесценивания.
1.21 UL2 [89]: Архитектура кодера-декодера, обучаемая с помощью смеси денуазеров (MoD). В число денуазеров входят: 1) R-денуазер: маскировка регулярного диапазона, 2) S-денуазер: повреждение последовательных лексем большой последовательности и 3) X-денуазер: случайное повреждение большого числа лексем. Во время предварительного обучения UL2 включает маркеры денойзера из R, S, X, чтобы представить настройку денойзера. Это помогает улучшить производительность тонкой настройки для последующих задач, которые связывают задачу с одним из режимов обучения вышестоящего потока. Этот стиль обучения MoD превосходит модель T5 во многих бенчмарках.
1.22 GLM-130B [33]: GLM-130B - двуязычная (английский и китайский языки) модель, обученная с помощью авторегрессивной задачи предварительного заполнения маски, аналогичной GLM [125]. Этот стиль обучения делает модель двунаправленной по сравнению с GPT-3, которая является однонаправленной. В отличие от GLM, обучение GLM-130B включает небольшое количество данных предварительного обучения многозадачной инструкции (5 % от общего количества данных) вместе с самоконтролируемым заполнением маски. Для стабилизации обучения применяется градиентное сокращение слоя встраивания.
1.23 LLaMA [126], [21]: Набор языковых моделей только для декодера, варьирующийся от 7B до 70B параметров. Серия LLaMA-моделей является наиболее известной в сообществе за эффективную настройку параметров и инструкций.LLaMA-1 [126]: Реализует эффективное каузальное внимание [127]за счет отказа от хранения и вычисления весов маскированного внимания и оценок ключей/запросов. Другой оптимизацией является уменьшение числа активаций, повторно вычисляемых при обратном проходе, как в [128].LLaMA-2 [21]: Эта работа в большей степени направлена на доработку более безопасной и качественной модели LLaMA-2-Chat для генерации диалогов. Предварительно обученная модель имеет на 40 % больше обучающих данных с большей длиной контекста и вниманием к сгруппированным запросам.
1.24 PanGu-Σ [129]: Авторегрессионная модель с параметрами, скопированными из PanGu-α и расширенными до триллионной шкалы со случайными экспертами (Random Routed Experts, RRE), архитектурная диаграмма которой показана на рис. 10. RRE похожа на архитектуру MoE с отличиями на втором уровне, где токены случайным образом направляются экспертам в области, а не с помощью метода обучаемого стробирования. В модели нижние слои активированы плотно и являются общими для всех доменов, в то время как верхние слои активируются редко в зависимости от домена. Такой стиль обучения позволяет извлекать модели, специфичные для конкретной задачи, и уменьшает количество катастрофических ошибок.

![[table_overview_llm.png]]





