![[overview_llm.png]]

A. Pre-Trained LLMs
Here, we provide summaries of various well-known pretrained LLMs with significant discoveries, changing the courseof research and development in NLP. These LLMs haveconsiderably improved the performance in NLU and NLGdomains, and are widely fine-tuned for downstream tasks.
1. General Purpose:
1.1 T5 [10]: An encoder-decoder model employing aunified text-to-text training for all NLP problems, shown inFigure 7. T5 places layer normalization outside the residualpath in a conventional transformer model [62]. It uses maskedlanguage modeling as a pre-training objective where spans(consecutive tokens) are replaced with a single mask instead ofseparate masks for each token. This type of masking speedsup the training as it produces shorter sequences. After pretraining, the model is fine-tuned using adapter layers [102]for downstream tasks.
1.2 GPT-3 [6]: The GPT-3 architecture is same as theGPT-2 [5] but with dense and sparse attention in transformerlayers similar to the Sparse Transformer [63]. It shows thatlarge models can train on larger batch sizes with a lowerlearning rate; in order to decide the batch size during training,GPT-3 uses the gradient noise scale as in [103]. Overall,GPT-3 increases model parameters to 175B showing that theperformance of large language models improves with the scaleand is competitive with the fine-tuned models.
1.3 mT5 [11]: A multilingual T5 model [10] trained onthe mC4 dataset with 101 languages. The dataset is extractedfrom the public common crawl scrape. The model uses alarger vocabulary size of 250,000 to cover multiple languages.To avoid over-fitting or under-fitting for a language, mT5employs a data sampling procedure to select samples from alllanguages. The paper suggests using a small amount of pretraining datasets, including all languages when fine-tuning fora task using English language data. This allows the model togenerate correct non-English outputs.
1.4 PanGu-α [104]: An autoregressive model that has aquery layer at the end of standard transformer layers, exampleshown in Figure 8, with aim to predict next token. Its structureis similar to the transformer layer but with an additionalembedding for the next position in the attention mechanism,given in Eq. 7
1.5 CPM-2 [12]: Cost-efficient Pre-trained languageModels (CPM-2) pre-trains bilingual (English and Chinese)11B and 198B mixture-of-experts (MoE) models on the WuDaoCorpus [105] dataset. The tokenization process removes“_” white space tokens in the sentencepiece tokenizer. Themodels are trained with knowledge inheritance, starting withonly the Chinese language in the first stage and then addingEnglish and Chinese data. This trained model gets duplicatedmultiple times to initialize the 198B MoE model. Moreover,to use the model for downstream tasks, CPM-2 experimentedwith both complete fine-tuning and prompt fine-tuning asin [106] where only prompt-related parameters are updatedby inserting prompts at various positions, front, middle, andback. CPM-2 also proposes INFMOE, a memory-efficientframework with a strategy to dynamically offload parametersto the CPU for inference at a 100B scale. It overlaps datamovement with inference computation for lower inferencetime.
1.6 ERNIE 3.0 [107]: ERNIE 3.0 takes inspiration frommulti-task learning to build a modular architecture usingTransformer-XL [108] as the backbone. The universal representation module is shared by all the tasks, which serve as thebasic block for task-specific representation modules, which areall trained jointly for natural language understanding, naturallanguage generation, and knowledge extraction. This LLM isprimarily focused on the Chinese language, claims to trainon the largest Chinese text corpora for LLM training, andachieved state-of-the-art in 54 Chinese NLP tasks.
1.7 Jurassic-1 [109]: A pair of auto-regressive languagemodels, including a 7B-parameter J1-Large model and a178B-parameter J1-Jumbo model. The training vocabulary ofJurassic-1 comprise word pieces, complete words, and multiword expressions without any word boundaries, where possibleout-of-vocabulary instances are interpreted as Unicode bytes.Compared to the GPT-3 counterparts, the Jurassic-1 modelsapply a more balanced depth-to-width self-attention architecture [110] and an improved tokenizer for a faster predictionbased on broader resources, achieving a comparable performance in zero-shot learning tasks and a superior performancein few-shot learning tasks given the ability to feed moreexamples as a prompt.
1.8 HyperCLOVA [111]: A Korean language model withGPT-3 architecture.
1.9 Yuan 1.0 [112]: Trained on a Chinese corpus with5TB of high-quality text collected from the Internet. AMassive Data Filtering System (MDFS) built on Spark isdeveloped to process the raw data via coarse and fine filteringtechniques. To speed up the training of Yuan 1.0 with theaim of saving energy expenses and carbon emissions, variousfactors that improve the performance of distributed trainingare incorporated in architecture and training like increasingthe number of hidden size improves pipeline and tensor parallelism performance, larger micro batches improve pipelineparallelism performance, and higher global batch size improvedata parallelism performance. In practice, the Yuan 1.0 modelperforms well on text classification, Winograd Schema, naturallanguage inference, and reading comprehension tasks.
1.10 Gopher [113]: The Gopher family of models rangesfrom 44M to 280B parameters in size to study the effect ofscale on the LLMs performance. The 280B model beats GPT3 [6], Jurrasic-1 [109], MT-NLG [114], and others on 81% ofthe evaluated tasks.
1.11 ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extendsERNIE 3.0 by training a larger model with 26x the number ofparameters of the latter. This bigger model outperformed otherstate-of-the-art models in 68 NLP tasks. LLMs produce textwith incorrect facts. In order to have control of the generatedtext with factual consistency, ERNIE 3.0 Titan adds anothertask, Credible and Controllable Generations, to its multitask learning setup. It introduces additional self-supervisedadversarial and controllable language modeling losses to thepre-training step, which enables ERNIE 3.0 Titan to beatother LLMs in their manually selected Factual QA task setevaluations.
1.12 GPT-NeoX-20B [115]: An auto-regressive modelthat largely follows GPT-3 with a few deviations in architecture design, trained on the Pile dataset without any data deduplication. GPT-NeoX has parallel attention and feed-forwardlayers in a transformer block, given in Eq. 8, that increasesthroughput by 15%. It uses rotary positional embedding [66],applying it to only 25% of embedding vector dimension asin [116]. This reduces the computation without performancedegradation. Opposite to GPT-3, which uses dense and sparselayers, GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult; therefore, the modelchooses hyperparameters from the method [6] and interpolatesvalues between 13B and 175B models for the 20B model. Themodel training is distributed among GPUs using both tensorand pipeline parallelism.
1.13 OPT [14]: It is a clone of GPT-3, developed withthe intention to open-source a model that replicates GPT-3performance. Training of OPT employs dynamic loss scaling[117] and restarts from an earlier checkpoint with a lowerlearning rate whenever loss divergence is observed. Overall,the performance of OPT-175B models is comparable to theGPT3-175B model.
1.14 BLOOM [13]: A causal decoder model trainedon ROOTS corpus with the aim of open-sourcing an LLM.The architecture of BLOOM is shown in Figure 9, withdifferences like ALiBi positional embedding, an additionalnormalization layer after the embedding layer as suggestedby the bitsandbytes1library. These changes stabilize trainingwith improved downstream performance.
1.15 GLaM [118]: Generalist Language Model (GLaM)represents a family of language models using a sparsely activated decoder-only mixture-of-experts (MoE) structure [119],[120]. To gain more model capacity while reducing computation, the experts are sparsely activated where only the besttwo experts are used to process each input token. The largestGLaM model, GLaM (64B/64E), is about 7× larger than GPT3 [6], while only a part of the parameters is activated per inputtoken. The largest GLaM (64B/64E) model achieves betteroverall results as compared to GPT-3 while consuming onlyone-third of GPT-3’s training energy.
1.16 MT-NLG [114]: A 530B causal decoder based onGPT-2 architecture that is roughly 3× GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collectedfrom various public datasets and blends various types ofdatasets in a single batch, which beats GPT-3 on a numberof evaluations.
1.17 Chinchilla [121]: A causal decoder trained on thesame dataset as the Gopher [113] but with a little differentdata sampling distribution (sampled from MassiveText). Themodel architecture is similar to the one used for Gopher,with the exception of AdamW optimizer instead of Adam.Chinchilla identifies the relationship that model size shouldbe doubled for every doubling of training tokens. Over 400language models ranging from 70 million to over 16 billionparameters on 5 to 500 billion tokens are trained to get theestimates for compute-optimal training under a given budget.The authors train a 70B model with the same compute budgetas Gopher (280B) but with 4 times more data. It outperformsGopher [113], GPT-3 [6], and others on various downstreamtasks, after fine-tuning.
1.18 AlexaTM [122]: An encoder-decoder model, whereencoder weights and decoder embeddings are initialized witha pre-trained encoder to speedup training. The encoder staysfrozen for initial 100k steps and later unfreezed for end-to-endtraining. The model is trained on a combination of denoisingand causal language modeling (CLM) objectives, concatenating [CLM] token at the beginning for mode switiching.During training, the CLM task is applied for 20% of the time,which improves the in-context learning performance.
1.19 PaLM [15]: A causal decoder with parallel attention and feed-forward layers similar to Eq. 8, speeding uptraining 15 times faster. Additional changes to the conventional transformer model include SwiGLU activation, RoPEembeddings, multi-query attention that saves computation costduring decoding, and shared input-output embeddings. Duringtraining, loss spiking was observed, and to fix it, modeltraining was restarted from a 100 steps earlier checkpointby skipping 200-500 batches around the spike. Moreover, themodel was found to memorize around 2.4% of the trainingdata at the 540B model scale, whereas this number was lowerfor smaller models.PaLM-2 [123]: A smaller multi-lingual variant of PaLM,trained for larger iterations on a better quality dataset. ThePaLM-2 shows significant improvements over PaLM, whilereducing training and inference costs due to its smaller size.To lessen toxicity and memorization, it appends special tokenswith a fraction of pre-training data, which shows reduction ingenerating harmful responses.
1.20 U-PaLM [124]: This method trains PaLM for 0.1%additional compute with UL2 (also named as UL2Restore)objective [89] using the same dataset and outperforms baselinesignificantly on various NLP tasks, including zero-shot, fewshot, commonsense reasoning, CoT, etc. Training with UL2Rinvolves converting a causal decoder PaLM to a non-causaldecoder PaLM and employing 50% sequential denoising, 25%regular denoising, and 25% extreme denoising loss functions.
1.21 UL2 [89]: An encoder-decoder architecture trainedusing a mixture of denoisers (MoD) objectives. Denoisersinclude 1) R-Denoiser: a regular span masking, 2) S-Denoiser:which corrupts consecutive tokens of a large sequence and3) X-Denoiser: which corrupts a large number of tokensrandomly. During pre-training, UL2 includes a denoiser tokenfrom R, S, X to represent a denoising setup. It helps improvefine-tuning performance for downstream tasks that bind thetask to one of the upstream training modes. This MoD styleof training outperforms the T5 model on many benchmarks.
1.22 GLM-130B [33]: GLM-130B is a bilingual (English and Chinese) model trained using an auto-regressivemask infilling pre-training objective similar to the GLM [125].This training style makes the model bidirectional as comparedto GPT-3, which is unidirectional. Opposite to the GLM, thetraining of GLM-130B includes a small amount of multi-taskinstruction pre-training data (5% of the total data) along withthe self-supervised mask infilling. To stabilize the training, itapplies embedding layer gradient shrink.
1.23 LLaMA [126], [21]: A set of decoder-only language models varying from 7B to 70B parameters. LLaMAmodels series is the most famous among the community forparameter-efficient and instruction tuning.LLaMA-1 [126]: Implements efficient causal attention [127]by not storing and computing masked attention weights andkey/query scores. Another optimization is reducing number ofactivations recomputed in backward pass, as in [128].LLaMA-2 [21]: This work is more focused towards finetuning a safer and better LLaMA-2-Chat model for dialoguegeneration. The pre-trained model has 40% more training datawith a larger context length and grouped-query attention.
1.24 PanGu-Σ [129]: An autoregressive model withparameters copied from PanGu-α and extended to a trillionscale with Random Routed Experts (RRE), the architecturaldiagram is shown in Figure 10. RRE is similar to the MoEarchitecture, with distinctions at the second level, where tokensare randomly routed to experts in a domain instead of using alearnable gating method. The model has bottom layers denselyactivated and shared across all domains, whereas top layers aresparsely activated according to the domain. This training styleallows extracting task-specific models and reduces catastrophic
